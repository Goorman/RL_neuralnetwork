{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class env_wrapper():\n",
    "    def __init__(self, game_title, actions):\n",
    "        self.state_size = 8\n",
    "        self.game_title = game_title\n",
    "        self.actions = actions\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Wrong game title.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_actions = 4\n",
    "action_names = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\",\n",
    "                \"FIRE_RIGHT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAME_TITLE = \"LunarLander-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AV_qnetwork():\n",
    "    def __init__(self, h_size, n_actions):\n",
    "        self.scalarInput = tf.placeholder(shape=[None, 8],dtype=tf.float32)\n",
    "        self.IW = tf.Variable(tf.random_normal([8,h_size]))\n",
    "        self.inp = tf.matmul(self.scalarInput,self.IW)\n",
    "        \n",
    "        self.streamAC,self.streamVC = tf.split(1,2,self.inp)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size/2, n_actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size/2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,n_actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lparams = {\"batch_size\" : 35, \n",
    "           \"MQN_updatefreq\" : 6,\n",
    "           \"TQN_updatefreq\" : 6000,\n",
    "           \"saver_freq\" : 100,\n",
    "           \"y\" : 0.99,\n",
    "           \"startE\" : 1,\n",
    "           \"endE\" : 0.1,\n",
    "           \"anneling_steps\" : 50000,\n",
    "           \"num_episodes\" : 1,\n",
    "           \"pre_train_steps\" : 50000,\n",
    "           \"load_model\" : False,\n",
    "           \"path\" : \"./dqn\",\n",
    "           \"h_size\" : 256, \n",
    "           \"gamma\" : 0.99,\n",
    "           \"render\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env):\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.saver_freq = lparams[\"saver_freq\"]\n",
    "        self.y = lparams[\"y\"]\n",
    "        self.startE = lparams[\"startE\"]\n",
    "        self.endE = lparams[\"endE\"]\n",
    "        self.anneling_steps = lparams[\"anneling_steps\"]\n",
    "        self.pre_train_steps = lparams[\"pre_train_steps\"]\n",
    "        self.load_model = lparams[\"load_model\"]\n",
    "        self.path = lparams[\"path\"]\n",
    "        self.h_size = lparams[\"h_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.render = lparams[\"render\"]\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self.mainQN = AV_qnetwork(self.h_size, self.env.n_actions)\n",
    "        self.targetQN = AV_qnetwork(self.h_size, self.env.n_actions)\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.trainables = tf.trainable_variables()\n",
    "        self.algBuffer = experience_buffer()\n",
    "\n",
    "        self.epsilon = self.startE\n",
    "        self.stepDrop = (self.startE - self.endE) / self.anneling_steps\n",
    "\n",
    "        self.jList = []\n",
    "        self.rList = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "            \n",
    "    def updateTarget(self, session):\n",
    "        total_vars = len(self.trainables)\n",
    "        for idx,var in enumerate(self.trainables[0:total_vars/2]):\n",
    "            session.run(self.trainables[idx + total_vars/2].assign(var.eval()))\n",
    "    \n",
    "    def discount_rewards(self, r):\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(xrange(0, r.size)):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r\n",
    "\n",
    "    def choose_action(self, state, session):\n",
    "        if np.random.rand(1) < self.epsilon or self.total_steps < self.pre_train_steps:\n",
    "            a = np.random.randint(0, self.env.n_actions)\n",
    "        else:\n",
    "            a = session.run(self.mainQN.predict,feed_dict={self.mainQN.scalarInput:[state]})[0]\n",
    "            \n",
    "        return a    \n",
    "    \n",
    "    def train(self, num_episodes, frame_limit):\n",
    "        with tf.Session() as sess:\n",
    "            if self.load_model == True:\n",
    "                print 'Loading Model...'\n",
    "                ckpt = tf.train.get_checkpoint_state(self.path)\n",
    "                self.saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            sess.run(self.init)\n",
    "            self.updateTarget(sess) \n",
    "            \n",
    "            for i in tqdm(range(num_episodes)):\n",
    "                episodeBuffer = experience_buffer()\n",
    "                state = self.env.make_reset()\n",
    "                \n",
    "                for iteration in xrange(0, frame_limit):\n",
    "                    action = self.choose_action(state, sess)\n",
    "                    new_state, reward, gameover, _ = self.env.make_step(action, self.render)\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    episodeBuffer.add(np.reshape(np.array([state, action, reward, new_state, gameover]),[1,5]))\n",
    "            \n",
    "                    if self.total_steps > self.pre_train_steps:\n",
    "                        if self.epsilon > self.endE:\n",
    "                            self.epsilon -= self.stepDrop\n",
    "            \n",
    "                        if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                            print \"Target network updated.\"\n",
    "                            self.updateTarget(sess)\n",
    "                \n",
    "                        if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                            trainBatch = self.algBuffer.sample(self.batch_size)\n",
    "                            Q1 = sess.run(self.mainQN.predict,feed_dict={self.mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                            Q2 = sess.run(self.targetQN.Qout,feed_dict={self.targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                            end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                            doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                            targetQ = trainBatch[:,2] + (self.y * doubleQ * end_multiplier)\n",
    "                            _ = sess.run(self.mainQN.updateModel, \\\n",
    "                                feed_dict={self.mainQN.scalarInput:np.vstack(trainBatch[:,0]),self.mainQN.targetQ:targetQ, self.mainQN.actions:trainBatch[:,1]})\n",
    "            \n",
    "                    state = new_state\n",
    "            \n",
    "                    if gameover:\n",
    "                        break\n",
    "    \n",
    "                episodeRewards = np.array(episodeBuffer.buffer)[:,2]\n",
    "                total_reward = np.sum(episodeRewards)\n",
    "                discountRewards = self.discount_rewards(episodeRewards)\n",
    "                bufferArray = np.array(episodeBuffer.buffer)\n",
    "                bufferArray[:,2] = discountRewards\n",
    "                self.algBuffer.add(zip(bufferArray))\n",
    "                self.jList.append(iteration)\n",
    "                self.rList.append(total_reward)\n",
    "                if i % self.saver_freq == 0:\n",
    "                    self.saver.save(sess,self.path+'/model-'+str(i)+'.cptk')\n",
    "                    print \"Saved Model\"\n",
    "                if len(self.rList) % 10 == 0:\n",
    "                    print \"total:\", self.total_steps, \" rlist:\", self.rList[-10:]\n",
    "                    print \"mean: \", np.mean(self.rList[-10:]), \" e: \", self.epsilon\n",
    "                    print self.jList[-10:]\n",
    "            self.saver.save(sess,self.path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-01 14:46:34,952] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "envhandle = env_wrapper(GAME_TITLE, action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, envhandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<29:38,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [00:15<27:23,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 889  rlist: [-120.32550079920317, -272.32822856698465, -255.61325231581989, -154.34624580734726, -170.07726667698006, -177.94052757979819, -169.68043348023764, -369.32183755348558, -156.6047513916663, -61.99741464945]\n",
      "mean:  -190.823545882  e:  1\n",
      "[66, 86, 60, 90, 100, 87, 79, 107, 91, 113]\n"
     ]
    }
   ],
   "source": [
    "ddql.train(num_episodes = 1000, frame_limit = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episodeBuffer.sample(1)[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pList = [np.average(rList[i:i+10]) for i in range(0, 790)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(pList)+1), pList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:    \n",
    "    print 'Loading Model...'\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "#    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess,\"./dqn3\"+'/model-'+str(1199)+'.cptk')\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(5)):\n",
    "        env.monitor.start('./tmp/carracing1-e' + str(i))\n",
    "        episodeBufferx = experience_buffer()\n",
    "        s1 = make_reset(env)\n",
    "        s = processState(s1)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0 \n",
    "        while (j < 1000):\n",
    "            j+=1\n",
    "            a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d,_ = make_step(env, a, render = True)\n",
    "            s = processState(s1)\n",
    "            rAll += r\n",
    "        env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
